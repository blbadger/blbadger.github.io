## Input Generation II: Vectorization and Latent Space Exploration

This page is part II on generating inputs using deep learning models trained for image classification. For part I, follow [this link](https://blbadger.github.io/input-generation.html)

### Introduction with Opposites

It is remarkable that deep learning models tasked with image classification are capable of producing coherent images representing a given output class. The task of image generation is far different than classification, but nevertheless recognizable images may be generated by optimizing the output for a given class.  In a previous section on this page, we also saw that images may be generated with a linear combination of two classes, which allowed us to transform a generated image of a husky into an image of a dalmatian.  

These observations lead to a natural idea: perhaps we can treat the output tensor of a deep learning model as a latent space. A latent space, also known as an embedding, is informally a space that captures meaningful information about the possible objects in that space.  More precisely, it is a manifold in which object similarity correlates with a distance metric on that manifold. 

We will consider the final 1000-dimensional fully connected layer activation of possible ImageNet categories as the output in question. At first glance it is not clear that this could be any kind of latent space: during training, each category is denoted by a one-hot vector in this space such that all possible categories are the same distance apart from each other.  This means that there is no prior information encoded in one output class versus another, which is exactly what one wants when training for classification without prior knowledge.

On the other hand, within this 1000-dimensional space we can view each class as a basis vector for this space and instead consider the possible vectors that exist in this space. A meaningful vector space of the outputs allows us to explore interesting questions by simply converting each question into a vector arithmetic operation.  

On a memorable episode of the popular comedy 'Seinfeld', the character George decides to do the opposite of what he would normally do with appropriately comedic results.  But one might wonder: what is the opposite?  For a number of ideas, there seems to be a natural opposite (light and dark, open and closed) but for others ideas or objects it is more difficult to identify an opposite: for example, what is the opposite of a mountian?  One might say a valley, but this is far from the only option.  Likewise, objects like a tree and actions like walking do not have clear opposites.

In [part I](https://blbadger.github.io/input-generation.html) we saw that deep learning models are capable of forming an image that represents some target output $\widehat y$.  This target output was usually a vector in which the entry at the index (signifying the ImageNet class) of choice was a large constant and zeros everywhere else, and the input image was modified using gradient descent in order to miminize the loss metric between the output and the target output.  With certain restrictions on how the gradient can be applied (smoothness in the input, for example) coherent and recognizable images can be generated.  Observation of the outputs shows that indeed the class of interest is maximized, as for example GoogleNet applied to maximize the activation of element 920 (signifying 'stoplight')

![opposite example]({{https://blbadger.github.io}}/neural_networks/negatives_example.png)

Observe that even though only index 920 was optimized, other output class activations have been affected as well.  It may be hypothesized that these activations correspond to a similarity between each of the 999 other ImageNet categories and class 920, with a higher activation signifying a more similar class where 'similar' is a measure on the function $f$ representing the GoogleNet model.  As a similarity metric this measure is actually quite effective (see the [below section](https://blbadger.github.io/latent_output.html#graphs-on-imagenet-classes) for more information) and it may be wondered whether the element with the smallest activation could be an accurate representation of an 'opposite' to the class being maximized.

More formally, we want to find the index $k$ 

$$
k = \mathrm{arg} \; \underset{i} {\mathrm{min}} \; O_i(a; \theta)
$$

where $a$ is an input generated to maximize some output class and $\theta$ denotes the configuration and parameters of the model used. 

The above method does not empirically yield very interesting results: the opposites of many ImageNet categories tend to be only a few classes, usually with no apparent relation to the category of interest. There is a clear theoretical basis for why this measure is not very effective: observe that there are many values that are near the minimum for the above image of a 'stoplight'.  It is not clear therefore that the index $k$ is well chosen, being that there is such a small difference between the outputs for many indicies.  Instead we want to find an index associated with a large distance between the value of the output at that index and the next smallest output. 

Finding a meaningful opposite using our image-generating procedure applied to deep learning models will not be difficult if the output is indeed a latent space.  We want to perform gradient descent on the input $a$ in order to minimize the activation of the output category of interest $O_i$, meaning that our loss function $J$ is

$$
J(O(a, \theta)) = O_i(a, \theta)
$$

and the gradient we want is the gradient of this loss with respect to the input, which is

$$
g = \nabla_a (O_i(a, \theta))
$$

The above formula can be implemented by simply assigning the loss to be the output of the output category as minimization is equivalent to maximization of a negative value.

```python
def layer_gradient(model, input_tensor, desired_output):
    ...
    input_tensor.requires_grad = True
    output = model(input_tensor)
    loss = output[0][int(desired_output)] # minimize the output class activation
    loss.backward()
    gradient = input_tensor.grad
    return gradient
```

and as before this gradient $g$ is used to perform gradient descent on the input, but now we will minimize rather than maximize the category of interest.

$$
a_{n+1} = a_n - \epsilon * g
$$

In geometric terms, this procedure is equivalent to the process of moving in the input space in a direction that corresponds with moving in the output space towards the negative axis of the dimension of the output category as far as possible.  

At first consideration, this procedure might not seem to be likely to yield any meaningful input $a_n$, as there is no guarantee that moving away from some class would not yield an input that is a mix of many different objects.  And indeed many generated opposite images are apparently a mix of a number of objects, for example this 'Piano' opposite that appears to be the image of a few different insects, or the opposite of 'Bonnet' that appears to be a mousetrap spring on fire.

![opposites]({{https://blbadger.github.io}}/neural_networks/googlenet_opposites_mix.png)

Despite it being unlikely that any of the 1000 ImageNet categories would have only one opposite, we can find the category of the image as classified by our model of choice (GoogleNet) by finding which element of the tensor of the model's output $O(a_n, \theta)$, denoted `output`, has the maximum activation.

```python
predicted = int(torch.argmax(output))
```

Now we can label each generated image according to which ImageNet category it most activates using a model of choice, here GoogleNet to be consistent with the image generation.  The following video shows the generation of an input $a$ that minimizes the GoogleNet activation for ImageNet class 55: Green Snake (red dot in the scatterplot to the right). Once again, two octaves with Gaussian convolutions are applied during gradient descent.

{% include youtube.html id='czayyaAi1cw' %}

Notice how a number of different categories have been maximized, and how the image appears to be a combination of different parts (an axolotl's gills with the feet and scales of a crocodile are perhaps the two most obvious).  Some objects have more coherent, even reasonable opposites: toilet paper is soft, flat, and waivy whereas syringes are thing and pointy.  

![opposites]({{https://blbadger.github.io}}/neural_networks/googlenet_opposites.png)

Dogs are perhaps the most interesting image category for this procedure nearly every ImageNet dog class has a coherent opposite that is also a dog, and the opposites generated seem to be logically motivated: observe how the opposites for large, long-haired dogs with no visible ears are small, thin, and perky-eared breeds.

![dog opposites]({{https://blbadger.github.io}}/neural_networks/googlenet_shaggy_opposites.png)

and likewise the opposites of a dog with longer fur and a pointed face (border collie) is one with short fur and squashed face (bloodhound), and the opposite of an image of a small dog with pointed ears (Ibizan hound) is a large dog with droopy ears (Tibetan Mastiff). Observe that opposites are rarely commutative: here we see a close but not quite commutative relation, where the opposite of an Ibizan is a Mastiff but the opposite of a Mastiff is a Terrier.  In general opposites are further from being commutative than this example.

![dog opposites]({{https://blbadger.github.io}}/neural_networks/googlenet_nonshaggy_opposites.png)

It is fascinating to see the generated images for the opposites of other animal classes.  

![animal opposites]({{https://blbadger.github.io}}/neural_networks/googlenet_animal_opposites.png)

The opposites of snakes are curiously usually lizards (including crocodiles) or amphibians (including axolotls) and the opposites of a number of birds are species of fish.  Opposites to all ImageNet class images according to GoogleNet may be found by following [this link](https://drive.google.com/drive/folders/1nE9X0PkG51RIL5euIHwOHfuV5OWWQm0i?usp=sharing). 

### Dog Transfiguration

In the last section, inputs representing the opposites of ImageNet classes were generated using gradient descent such that the gradient used to minimize the activation of the ImageNet class in question $g'$ was $g' = -g$, i.e. the gradient used to maximize the activation value multiplied by negative one.  As the negative value cancels with the gradient descent term $a_{i+1} = a_i - \epsilon * (-g)$ this procedure is sometimes called gradient ascent.  Multiplying by negative one is far from the only transformation we can perform, however: here we explore linear combinations of two target classes using InceptionV3, with a real image as a starting point.

We can view the difference between a Husky and a Dalmatian according to some deep learning model by observing what changes as our target class shifts from 'Husky' to 'Dalmatian', all using a picture of a dalmatian as an input.  To do this we need to be able to gradually shift the target from the 'Husky' class (which is $\widehat y_{250}$ in ImageNet) to the 'Dalmatian' class, corresponding to $\widehat y_{251}$.  This can be accomplished by assigning the loss $J_n(0(a, \theta))$ $n=q$ maximum interations, at iteration number $n$ as follows:

$$
J_n(O(a, \theta)) \\
= \left(c - \widehat y_{250} * \frac{q-n}{q} \right) + \left(c - \widehat y_{251} * \frac{n}{q} \right) 
$$

and to the sum on the right we can add an $L^1$ regularizer if desired, applied to either the input directly or the output.  Applied to the input, the regularizer is as follows:

$$
L_1 (a) = \sum_i \lvert a_i \rvert
$$

Using this method, we go from $(\widehat y_{250}, \widehat y_{251}) = (c, 0)$ to $(\widehat y_{250}, \widehat y_{251}) = (0, c)$ as $n \to q$.  The intuition behind this approach is that $(\widehat y_{250}, \widehat y_{251}) = (c/2, c/2)$ or any other linear combination of $c$ should provide a mix of characteristics between target classes. 

{% include youtube.html id='1bdpG1caKMk' %}

Using InceptionV3 as our model for this experiment, we have we see that this is indeed the case: observe how the fluffy husky tail becomes thin, dark spots form on the fur, and the eye color darkens as $n$ increases.

{% include youtube.html id='PBssSJoLOhU' %}

### Vector Addition and Subtraction

We have so far seen that it is possible to generate recognizable images $a'$ that represent the opposites of some original input $a$, where the gradient descent procedure makes the input $a' = -a$ according to how the model views each input.  Likewise it has been observed that linear combinations of the output corresponding to two breeds of dog yield recognizable images where $a' = ba_0 + ca_1$ for some constant $d$ such that $b + c = d$.

We can explore other vector operations.  Vector addition is the process of adding the component vectors in a space, and may be thought of as resulting in a vector that contains some of the qualities of both operands. One way to perform vector addition during gradient descent on the input is to perform each update $a' = a + \epsilon g$ such that the gradient is

$$
g = \nabla_a (C - O_1(a, \theta)) + \nabla_a(C - O_2(a, \theta)) \\
= \nabla_a (2C - O_1(a, \theta) - O_2(a, \theta)) 
$$

which leads to the appearance of merged objects, for example this turtle and snowbird

![resnet_addition]({{https://blbadger.github.io}}/neural_networks/vectorized_resnet_1.png)

This sort of addition we can call a 'merging', as characteristics of both target classes $a_1, a_0$ are found in the same contiguous object.  Merging is fairly common using the above gradient.

![resnet_addition]({{https://blbadger.github.io}}/neural_networks/vectorized_resnet_2.png)

Some target classes tend to make recognizable shapes of one but not both $a_0, a_1$.  Instead we can try to optimize the activation of these categories separately, choosing to optimize the least-activated neuron at each iteration. The gradient is therefore

$$
g' =
\begin{cases}
\nabla_a(C - O_2(a, \theta)),  & \text{if} \; O_1 \geq O_2 \\
\nabla_a(C - O_1(a, \theta)),  & \text{if} \; O_1 < O_2
\end{cases}
$$

This $g'$ more often than $g$ when applied to gradient descent does give an image which places both target objects are recognizable and separated in the final image, which we can call juxtaposition.

![resnet_addition]({{https://blbadger.github.io}}/neural_networks/vectorized_resnet_3.png)

However the addition is performed, there are instances in which the output is neither the merging nor juxtaposition of target class objects.  For example, (1) applied to addition of a snowbird to a tarantula yields an indeterminate image somewhat resembling a black widow.

![resnet_addition]({{https://blbadger.github.io}}/neural_networks/resnet_junco_tarantula.png)


### Feature Latent Space

Suppose one were to want to understand which of the ImageNet categories were more or less similar to another.  For example, is an image of a cat more similar to a fox or a wolf?  Specifically we want this question answered with abstract ideas like facial and tail structure, rather than some simple metric like color alone.

This question is not at all easy to address.  We seek a metric that will determine how far ImageNet category is from every other category, but the usual metrics one can place on an image will not be sufficient.  Perhaps the simplest way to get this metric is to take the average image for each category (by averaging the values of all images of one category pixel per pixel) and measure the $L^2$ or $L^1$ distance between each image.  This is almost certain to fail in our case because there is no guarantee that such a distance would correspond to higher-level characteristics rather than lower-level characteristics like color or hue.

Instead we want a measurement that corresponds to more abstract quantities, like the presence of eyes, number of legs, or roundness of an object in an image. We could use those three traits alone, and make a three-dimensional representation called an embedding consisting of points in space where the basis of the vector space is precisely the values attached to each of these characteristics.  For example, if we have some object where `[eyes, legs, roundness] = [4, 10, 0.2]` we would likely have some kind of insect, whereas the point `[-10, -2, 10]` would most likely be an inanimate object like a beach ball.

Happily for us, deep learning models are capable of observing high-level characteristics of an image.  We have seen that [feature maps](https://blbadger.github.io/feature-visualization.html) of certain hidden layers of these models tend to be activated by distinctive patterns, meaning that we can use the total or average activation of a feature map as one of our basis vectors.

Somewhat arbitrarily, let's choose two features from GoogleNet's layer 5a as our basis vectors.  For reference, here are the maps for the features of interest (meaning that the following images were found to maximally activate the features via gradient descent):

![resnet_addition]({{https://blbadger.github.io}}/neural_networks/googlenet_features_latent.png)

Feature 0 seems to respond to a brightly colored bird-like pattern whereas feature 4 is maximally activated by something resembling a snake's head and scales.  We can observe the activation of these layers for GoogleNet-generated images representing each ImageNet class in order to get an idea of which categories these layers score as more or less similar from each other.  The following code allows us to plot the embedding performed by these features by plotting the average activation of the two features for each generated output.

```python
def plot_embedding():
    x, y, labels_arr = [], [], []
    for i, image in enumerate(images):
        label = image[1]
        image = image[0].reshape(1, 3, 299, 299).to(device)
        output = network(image)
        x.append(float(torch.mean(output[0, 0, :, :])))
        y.append(float(torch.mean(output[0, 4, :, :])))
        i = 11
        while label[i] not in ',.':
            i += 1
        labels_arr.append(label[11:i])

    plt.figure(figsize=(18, 18))
    plt.scatter(x, y)
    for i, label in enumerate(labels_arr):
        plt.annotate(label, (x[i], y[i]))
    plt.show()
    plt.close()
    return
```

this yields

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/googlenet_5a_04_embedding.png)

which has noticeably skewed distribution,

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/googlenet_5a_distribution.png)

It appears that Feature 0 corresponds to a measure of something similar to 'brightly-colored bird' whereas Feature 4 is less clear but is most activated by ImageNet categories that are man-made objects.

### Graphs on ImageNet classes

Investigating which ImageNet categories are more or less similar than each other was explored in the previous section using two features from one layer of a chosen model (GoogleNet).  But in one sense, these embeddings are of limited use, because they represent only a very small portion of the information that the model possesses with regards to the input images, as there are many more features in that layer and many more layers in the model. To be specific, the embedding diagram in the last section denotes that 'Jay' is the ImageNet class most similar to 'Indigo Bunting' for GoogleNet, but only for two out of over 700 features of one specific layer.

Each of the features and layers are important to the final classification prediction, and moreover these layers and features are formed by non-linear functions such that the features and layers are non-additive.  Therefore although the embeddings of the output categories using feature activation as the basis space is somewhat useful, it is by no means comprehensive.  Another approach may be in order in which the entire model is used, rather than a few features.

There does exist a straightforward way to determine which ImageNet categories are more or less similar than each other: we can simply take the output vector $y = O(a', \theta)$ and observe the magnitudes of the components of this vector.  
There exists a problem with using the this approach as a true similarity metric, however: $y = O(a', \theta)$ is not guaranteed to be symmetric, or in other words generally $m(a, b) \neq m(b, a)$.  This means that we cannot use the findings from the generation metric to make a vector space or any other metric space as the allowable definition of a distance metric is not followed.  

But because pairs of points exhibit an asymmetric measurement, we cannot portray this as a metric space.  But it is possible to portray these points as an abstract graph, with nodes corresponding to ImageNet categories (ie outputs) and verticies corresponding to relationships between them.  We will start by only plotting the 'nearest neighbor' relationship, which is defined as the output that is most activated by the generated image distinct from the target output $\widehat y$.  

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/nearest_neighbor_explanation.png)

```python
import networkx as nx

def graph_nearest():
    # convert array of pairs of strings to adjacency dict
    closest_dict = {}
    for pair in closest_array:
        if pair[0] in closest_dict:
            closest_dict[pair[0]].append(pair[1])
        else:
            closest_dict[pair[0]] = [pair[1]]

    G = nx.Graph(closest_dict)
    nx.draw_networkx(G, with_labels=True, font_size=12, node_size=200, node_color='skyblue')
    plt.show()
    plt.close()
    return
```

The first half of the 1000 ImageNet categories are mostly animals, and plotting a graphs for them yields

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/nearest_neighbors_animal_embedding.png)

Nodes that are connected together form a 'component' of the graph, and nodes that are all connected to each other form a complete component called a 'clique'.  Cliques of more that two nodes are extremely rare for ImageNet nearest neighbors, but non-trivial (ie those with more than two nodes) components abound, often with very interesting and logical structures.  Observe how cats form one component, and terrier dogs preside in another, and mustelids and small mammals in another

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/animal_components.png)

For the non-animal half of ImageNet, graphing nearest neighbors yields a component with more members than was observed for any animal component.

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/nearest_neighbors_nonanimal_embedding.png)

This contains many diverse objects, yet often still exhibit relationships that seem reasonable to a human.

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/nonanimal_components1.png)

Smaller components are often the most illuminating: observe the sports balls clustering together in one component, and the utensils in another component

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/nonanimal_components2.png)

### Output Latent Space

The above measurement is illuminating but is not a true metric space.  Instead of finding the second largest output for our generated inputs, we can find the ImageNet class corresponding to the second closest point (including the point of interest) to our point in the output space. This means that we wish to perform an embedding of the output with respect to the model.  The reasoning here is that our trained GoogleNet model with parameters $\theta$ may be viewed as a (very complicated) function $O$ that maps input images $a$ to an output $y$, which is a 1000-dimensional vector where the element of index $n$ denoted by $y_n$.

$$
y = O(a, \theta)
$$

Now if we use real images from some category, there is no guarantee that $v$ will be unchanged and it is unclear which $v$ best respresents the image category.  Instead we can use an image $a'$ generated to maximize the output category of interest (for more information on this, see [here](https://blbadger.github.io/input-generation.html)) as an approximation of the input that will most resemble the output category of interest $\widehat y_n = O_n(a, \theta)$.

$$
a' = \mathrm{arg} \; \underset{a}{\mathrm{max}} \; O_n(a, \theta)
$$

Using these representative inputs $a'$ applied to $O$, we can find the coordinates of all model outputs $y_n \in \Bbb R^{1000}$.  This means that we can find the coordinates of the representative input $a'$ in the 1000-dimensional output space. For a three-dimensional example of this method see the following figure.  Note that the metric $m(y_1, y_2)$ may be chosen from any number of possible methods.

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/vector_output_explanation.png)

As spaces with more than two or three dimensions are hard to visualize, we can perform a dimensionality reduction method for visualization, and here we will find a function $f$ to take $f: y_n \in \Bbb R^{1000} \to z_n \in \Bbb R^2$.  We shall employ principle component analysis, which is defined as the function $f(y)$ that produces the embedding $z$ such that a decoding function $g$ such that $x \approx g(f(y))$, where $g(y) = Dy$ and $D \in \Bbb R^{1000x2}$.  Therefore PCA is defined as the encoding function $f$ that minimized the distance of the encoded value $z$ from the original value $y$ subjected to the constraint that the decoding process be a matrix multiplication.  To further simplify things, $D$ is constrained to have linearly independent columns of unit norm.  The minimization procedure may be accomplished using eigendecomposition and does not requre gradient descent.

When we find the coordinates of $y_n$ for all $n$ ImageNet categories using GoogleNet and then map these points using the first two principle components

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/googlenet_output_embedding.png)

but the result is somewhat underwhelming.  Principle conponents 1 and 2 account for only $15$ and $4$ percent (respectively) of the variance which means that they are nearly meaningless as they capture very little of the original distribution.

Why is this the case?  The failure lies in PCA's expectation of a linear space, in which transformations $f$ are additive and scaling

$$
af(x + y) = f(ax) + f(ay)
$$

and where in particular the intuitive metric of distance stands.  As points in this space were generated using a nonlinear function (gradient descent of GoogleNet on a scaled normal input), there is no reason to think that a linear decomposition would be capable of capturing much of the variance in that function.

### Model Similarity Vector Space

In [part I](https://blbadger.github.io/input-generation.html#inceptionv3-and-googlenet-compared), it was observed that different models generate slightly different representations of each ImageNet input class.  It may be wondered whether or not we can use the gradients of two models to make an input, perhaps by combining the gradients of a model with parameters $\theta_1$ and another with parameters $\theta_2$ to make a gradient $g'$

$$
g' = \nabla_a \left( d*(C - O_i(a, \theta_1)) + e*(C - O_i(a, \theta_2)) \right)
$$

where $d, e$ are constants used to scale the gradients of models $\theta_1, \theta_2$ appropriately.  For GoogleNet and Resnet, we approximate $d = 2, e = 1$ by applying the constant only to $C$ as follows (note that there is little difference in applying the constant to the gradient or the constant alone)

```python
def layer_gradient(model, input_tensor, desired_output):
    ...
    loss = 0.5*((200 - output[0][int(desired_output)]) + (400 - output2[0][int(desired_output)]))
    ...
```

![googlenet and resnet input]({{https://blbadger.github.io}}/neural_networks/combined_mousetrap.png)

Images of all 1000 ImageNet classes generated using the combined gradient of GoogleNet and ResNet are available [here](https://drive.google.com/drive/folders/1mhj8vBm02Fd6QkQwEQ4jhI9yq34ZOtR0?usp=sharing). From these images it is clear that the combined gradient is as good as or superior to the gradients from only ResNet or GoogleNet with respect to producing a coherent input image, which suggests that the gradients from these models are not substantially dissimilar.

The above observation motivates the following question: can we attempt to understand the differences between models by generating an image representing the difference between the output activations for any image $a$? 

It is also apparent that the similarities and differences in model output may be compared by viewing the output as a vector space.  Say two models were to give very similar outputs for a representation of one ImageNet class but different outputs for another class. The identities of the classes may help inform an understanding of the the difference between models.

### Model Merging

In the last section it was observed that we can understand some of the similarities and differences between models by viewing the output as a vector space, with each model's output on each ImageNet representation being a point in this space.

What if we want one model to generate a representation of an ImageNet class that is similar to another model's representation?  We have already seen that some models (GoogleNet and Resnet) generally yield recognizable input representations whereas others (InceptionV3) yield somewhat less-recognizable inputs.  But if we were stuck with only using InceptionV3 as our image-generation source, can we try to use some information present from the other models in order to generate a more-recognizalbe image?

One may hypothesize that it could be possible to train one model to become 'more like' another model using the output of the second as the target for the output of the first.  Consider one standard method of training using maximum likelihood estimation via minimization of cross-entropy betwen the true output $Q$ and the model output $Q$ given input $x$,

$$
J(O(a, \theta), \widehat y) = H(P, Q) = -\Bbb E_{x \sim P} \log Q(x)
$$

The true output $Q$ is usually denoted as a one-hot vector, ie for an image that belongs to the first ImageNet category we have $[1, 0, 0, 0...]$.  A cross-entropy loss function measures the similarity between the output distribution model output $Q$ and this one-hot tensor $Q$.  

But there are indications that this is not an optimal training loss.  Earlier on this page we have seen that for trained models, some ImageNet categories are more similar with respect to the output activations $O(a, \theta)$ to some other ImageNet categories, and different from others.  These similarities moreover are by a nearest neighbors graphical representation intuitive for a human observer, meaning that it is indeed likely that some inputs are more similar than others.

Training requires the separation of the distributions $P_1, P_2, ... P_n$ for each imagenet category $n$ in order to make accurate predictions. But if we have a trained model that has achieved sufficient separation, the information that the model has difficulty separating certain images from others (meaning that these are more similar ImageNet categories by our output metric) is likely to be useful information in training a model.  This information is not likely to be found prior to training, and thus is useful for transfer learning.


These observations motivate the hypothesis that we may be able to use the information present in the output of one model $\theta_1$ to train another model $\theta_2$ to be able to represent an input in a similar way to $\theta_1$.  More precisely, one can use gradient descent on the parameters of model 1, $\theta_1$ to make some metric between the outputs $m(O(a, \theta_1), O(a, \theta_2))$ as small as possible. In the following example, we seek to minimize  the sum-of-squares residual as our metric

$$
J(O(a, \theta_1) = L_{RSS} = \sum_i \left(O_i(a, \theta_1) - O_i(a, \theta_2) \right)^2 
$$

which can be implemented as

```python
def train(model, input_tensor, target_output):
    ...
    output = model(input_tensor)
    loss = torch.sum(torch.abs(output - target_output)**2) # sum-of-squares loss
    optimizer.zero_grad() # prevents gradients from adding between minibatches
    loss.backward()
    optimizer.step()
    return 
```
Note that more commonly-used metrics like $L^1$ or $L^2$ empirically do not lead to significantly different results as our RSS metric here.  Whichever objective function is chosen, it can be minimized by gradient descent on the model parameters given an input image $a$ is

$$
\theta_1^' = \theta_1 + \epsilon * \nabla_{\theta_1} J(O(a, \theta_1))
$$

which may be implemented as

```python
def gradient_descent():
    optimizer = torch.optim.SGD(resnet.parameters(), lr=0.00001)
    # access image generated by GoogleNet
    for i, image in enumerate(images):
        break 
    image = image[0].reshape(1, 3, 299, 299).to(device)
    target_output = googlenet(image).detach().to(device)
    target_tensor = resnet(image) - target_output
    for _ in range(1000):
        train(resnet, image, target_output)
```

Gradient descent is fairly effective at reducing a chosen metric between ResNet $\theta_1$ and GoogleNet $\theta_2$.  For example, given $a$ to be GoogleNet's representation of Class 0 (Tench), $J(O(a, \theta_1))$ decreases by a factor of around 15.  But the results are less than impressive: applying gradient descent to ResNet's parameters as $\theta_1$ to reduce the sum-of-squares distance between this model's output and GoogleNet's output does not yield a model that is capable of accurately representing this class.

It may be hypothesized that this could be because although ResNet's outputs match GoogleNet's for this class, each class has a different 'meaning', ie latent space location, which would undoubtedly hinder our efforts here.  But even if we repeat this procedure to train ResNet's outputs match GoogleNet's(for that model's representations) for all 1000 ImageNet classes, we still do not get an accurate representation of Class 0 (or any other class of interest).

![resnet trained to be googlenet tench]({{https://blbadger.github.io}}/neural_networks/resnet_trained_to_be_googlenet.png)

It is certainly possible that this method would be much more successful if applied to natural images rather than generated representations.  But this would go somewhat against the spirit of this hypothesis because natural images would bring with them new information that may not exist in the models $\theta_1, \theta_2$ even if the images come from the ImageNet training set.

These results suggest that modifying $\theta_1$ to yield and output that matches that of $\theta_2$ is a somewhat difficult task, at least if we limit ourselves to no information that is not already present in the model.  But this could simply be due to known difficulties in model training rather than an inability to explore the output latent space.  

Therefore a more direct approach to making one model yield another model's representations: rather than modifying the first model's parameters $\theta_1$ via gradient descent, we instead find the coordinates of the second model's representation of a given class in our output space and then use these coordinates as a target for gradient descent on an initially randomized input $a_0$.

For clarity, this procedure starts with an image representing the representation of a certain class with respect to some model $\theta_2$, denoted $a_{\theta_2}$.  We want to have a different model $\theta_1$ learn how to generate an approximation of this representation with no outside information. First we find a target vector $\widehat{y} \in \Bbb R^n$, with $n$ being the number of possible outputs, defined as

$$
\widehat{y} = O(a_{\theta_2}, \theta_1) 
$$

or in words the target vector is the output of the model of interest $\theta_1$ when the input supplied is the representation of some class for the model we want to emulate.

Now rather than performing gradient descent using a target $C \Bbb R^n$ being a vector with a large constant in the appropriate index for our class of interest and zeros elsewhere, we instead try to reduce the distance between $O(a_0, \theta_1)$ and the target vector $\widehat{y}$. Again this distance can be any familiar metric, perhaps $L^1$ which makes the computation as follows:

$$
g = \nabla_a \sum_n \lvert \widehat{y_n} - O_n(a, \theta_1) \rvert \\
$$

with the input modification being a modified version of gradient descent using smoothness (ie pixel cross-correlation) via Gaussian convolution $\mathcal{N}$ and translational invariance with Octave-based jitter, here denoted $\mathscr{J}$, were employed with the gradient for input $a_n$, denoted $g_n$.  The actual update procedure is

$$
a_{n+1} =\mathscr{J} \left( \mathcal{N}(a_n + \epsilon g_n) \right)
$$

where the initial input $a_0$ is a scaled random normal distribution. This gradient can be implemented as follows:

```python
def layer_gradient(model, input_tensor, desired_output):
    ...
    input_tensor.requires_grad = True
    output = resnet(input_tensor)
    loss = 0.05*torch.sum(torch.abs(target_tensor - output))
    loss.backward()
    gradient = input_tensor.grad
```

If this method is successful, it would suggest that our model of interest $\theta_1$ has the capability to portray the representation that another model $\theta_2$ has of some output class, and that this portrayal may be made merely by finding the right point in the output space.  And indeed for Class 0 this is the case: observe how the output on the right mimicks GoogleNet's representation of a Tench.

![resnet vectorized to be googlenet tench]({{https://blbadger.github.io}}/neural_networks/resnet_vectorized_to_be_googlenet.png)

The ability of the right point in output space to mimick the representation by another model (for some given class) is even more dramatic when the model's representations of that class are noticeably different.  For example, observe the representation of 'Class 11: Goldfinch' by ResNet and GoogleNet in the images on the left and center below.  ResNet (more accurately) portrays this class using a yellow-and-black color scheme with a dark face whereas GoogleNet's portrayal has reddish feathers and no dark face.  But if we perform the above procedure to ResNet, it too mimicks the GoogleNet output.

![resnet vectorized to be googlenet goldfinch]({{https://blbadger.github.io}}/neural_networks/resnet_vectorized_to_be_googlenet_goldfinch.png)























